{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Aim: Implementing State Action Reward  State action (SARSA) algorithm using python and compare it with Q Learning."
      ],
      "metadata": {
        "id": "JmqpY_AMepxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NutmcW5YbFgd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class GridWorld:\n",
        "    def __init__(self, rows, cols, start, goal, obstacles):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.obstacles = obstacles\n",
        "        self.state = start\n",
        "        self.is_terminal = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start\n",
        "        self.is_terminal = False\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = tuple(np.array(self.state) + np.array(action))\n",
        "\n",
        "        if next_state == self.goal:\n",
        "            reward = 1\n",
        "            self.is_terminal = True\n",
        "        elif next_state in self.obstacles or not (0 <= next_state[0] < self.rows) or not (0 <= next_state[1] < self.cols):\n",
        "            reward = -1\n",
        "            self.is_terminal = False\n",
        "        else:\n",
        "            reward = 0\n",
        "            self.state = next_state\n",
        "            self.is_terminal = False\n",
        "\n",
        "        return next_state, reward, self.is_terminal\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SARSAAgent:\n",
        "    def __init__(self, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.actions = actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_values = {}\n",
        "    def choose_action(self, state):\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            action_index = np.random.choice(len(self.actions))\n",
        "            return self.actions[action_index]\n",
        "        else:\n",
        "            q_vals = [self.get_q_value((state, a)) for a in self.actions]\n",
        "            return self.actions[np.argmax(q_vals)]\n",
        "\n",
        "    def get_q_value(self, sa_pair):\n",
        "        return self.q_values.get(sa_pair, 0)\n",
        "\n",
        "    def update_q_value(self, state_action, new_q_value):\n",
        "        self.q_values[state_action] = new_q_value\n",
        "\n",
        "    def print_q_values(self):\n",
        "        print(\"Q-values Table:\")\n",
        "        for state_action, value in self.q_values.items():\n",
        "            state, action = state_action\n",
        "            print(f\"State: {state}, Action: {action}, Q-value: {value}\")\n"
      ],
      "metadata": {
        "id": "OZhAPqNRcH3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sarsa(agent, environment, episodes):\n",
        "    for episode in range(episodes):\n",
        "        environment.reset()\n",
        "        state = environment.state\n",
        "        action = agent.choose_action(state)\n",
        "        while not environment.is_terminal:\n",
        "            next_state, reward, is_terminal = environment.step(action)\n",
        "            next_action = agent.choose_action(next_state)\n",
        "            next_state_action = (next_state, next_action)\n",
        "            agent.update_q_value((state, action), (1 - agent.alpha) * agent.get_q_value((state, action)) + agent.alpha * (reward + agent.gamma * agent.get_q_value(next_state_action)))\n",
        "            state, action = next_state, next_action\n"
      ],
      "metadata": {
        "id": "E317RsDOcQ2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    rows, cols = 4, 4\n",
        "    start = (0, 0)\n",
        "    goal = (3, 3)\n",
        "    obstacles = [(1, 1), (2, 1), (2, 2)]\n",
        "\n",
        "    environment = GridWorld(rows, cols, start, goal, obstacles)\n",
        "    actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up\n",
        "\n",
        "    sarsa_agent = SARSAAgent(actions)\n",
        "\n",
        "    episodes = 1000\n",
        "    train_sarsa(sarsa_agent, environment, episodes)\n",
        "\n",
        "    # Print the learned Q-values for SARSA\n",
        "    sarsa_agent.print_q_values()\n",
        "\n",
        "    # Test the trained SARSA agent\n",
        "    environment.reset()\n",
        "    state = environment.state\n",
        "    steps = 0\n",
        "\n",
        "    while not environment.is_terminal and steps < 20:\n",
        "        action = sarsa_agent.choose_action(state)\n",
        "        next_state, _, _ = environment.step(action)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"SARSA Agent reached the goal in {steps} steps.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdFmnWTFcUmD",
        "outputId": "a25f7ecf-8c62-43f3-cda5-a584dfc6bff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values Table:\n",
            "State: (0, 0), Action: (0, 1), Q-value: 0.03747798427751295\n",
            "State: (0, 1), Action: (0, 1), Q-value: -0.037713090507376895\n",
            "State: (0, 2), Action: (0, 1), Q-value: -0.0014980724010000012\n",
            "State: (0, 3), Action: (0, 1), Q-value: -0.271\n",
            "State: (0, 4), Action: (0, 1), Q-value: -0.1\n",
            "State: (0, 4), Action: (0, -1), Q-value: -0.00036082463647567104\n",
            "State: (0, 3), Action: (0, -1), Q-value: -0.0007290000000000002\n",
            "State: (0, 2), Action: (1, 0), Q-value: -0.009315620662732966\n",
            "State: (1, 2), Action: (0, 1), Q-value: -0.008100000000000001\n",
            "State: (1, 3), Action: (0, 1), Q-value: -0.19\n",
            "State: (1, 4), Action: (0, 1), Q-value: -0.19\n",
            "State: (1, 4), Action: (0, -1), Q-value: 0.0\n",
            "State: (1, 3), Action: (0, -1), Q-value: -3.0192994488239993e-05\n",
            "State: (1, 3), Action: (-1, 0), Q-value: 0.0\n",
            "State: (0, 3), Action: (1, 0), Q-value: 0.016683084000000004\n",
            "State: (1, 2), Action: (1, 0), Q-value: -0.19\n",
            "State: (2, 2), Action: (0, 1), Q-value: 0.9289280992185065\n",
            "State: (1, 4), Action: (1, 0), Q-value: 0.0\n",
            "State: (2, 3), Action: (0, 1), Q-value: -0.18100000000000002\n",
            "State: (2, 4), Action: (0, 1), Q-value: -0.19\n",
            "State: (2, 4), Action: (0, -1), Q-value: -0.1\n",
            "State: (2, 4), Action: (1, 0), Q-value: 0.19\n",
            "State: (0, 2), Action: (-1, 0), Q-value: -0.40951000000000004\n",
            "State: (-1, 2), Action: (0, 1), Q-value: -0.0171\n",
            "State: (0, 2), Action: (0, -1), Q-value: -0.01687661499526096\n",
            "State: (1, 2), Action: (0, -1), Q-value: -0.1\n",
            "State: (1, 1), Action: (0, 1), Q-value: -0.36172171000000003\n",
            "State: (1, 2), Action: (-1, 0), Q-value: -0.0012861331603941603\n",
            "State: (0, 1), Action: (0, -1), Q-value: 0.20040492744977081\n",
            "State: (0, 0), Action: (1, 0), Q-value: 0.4528642341693484\n",
            "State: (1, 0), Action: (0, 1), Q-value: -1.0265502622203948\n",
            "State: (1, 1), Action: (0, -1), Q-value: -0.4445010665532826\n",
            "State: (1, -1), Action: (0, 1), Q-value: -0.21919362172396772\n",
            "State: (1, 1), Action: (1, 0), Q-value: -0.45723610000000003\n",
            "State: (2, 0), Action: (0, 1), Q-value: -0.9386472845946743\n",
            "State: (2, 1), Action: (0, 1), Q-value: -0.2762878340436525\n",
            "State: (2, 1), Action: (0, -1), Q-value: -0.22635473358179303\n",
            "State: (2, -1), Action: (0, 1), Q-value: -0.1\n",
            "State: (2, 1), Action: (1, 0), Q-value: 0.09734212429744135\n",
            "State: (3, 0), Action: (0, 1), Q-value: 0.6001909338109113\n",
            "State: (3, 1), Action: (0, 1), Q-value: 0.7919733356668425\n",
            "State: (3, 2), Action: (0, 1), Q-value: 0.9999999999999994\n",
            "State: (0, 0), Action: (0, -1), Q-value: -0.8485368158328592\n",
            "State: (0, -1), Action: (0, 1), Q-value: -0.0013413402276261394\n",
            "State: (1, 0), Action: (-1, 0), Q-value: 0.21698424067322628\n",
            "State: (0, 1), Action: (1, 0), Q-value: -0.5732717055016019\n",
            "State: (1, 1), Action: (-1, 0), Q-value: 0.0020539516556363734\n",
            "State: (-1, 1), Action: (0, 1), Q-value: -0.022308239889000006\n",
            "State: (0, 1), Action: (-1, 0), Q-value: -0.4119044457435217\n",
            "State: (2, 2), Action: (0, -1), Q-value: -0.04036952018619356\n",
            "State: (2, 2), Action: (1, 0), Q-value: -0.19\n",
            "State: (2, 2), Action: (-1, 0), Q-value: -0.1970915806508104\n",
            "State: (1, 0), Action: (0, -1), Q-value: -0.6791816112255882\n",
            "State: (1, -1), Action: (1, 0), Q-value: 0.41468908875626637\n",
            "State: (2, 0), Action: (0, -1), Q-value: -0.7164486140954296\n",
            "State: (2, -1), Action: (0, -1), Q-value: -0.19\n",
            "State: (2, -1), Action: (1, 0), Q-value: 0.0\n",
            "State: (1, 0), Action: (1, 0), Q-value: 0.5229895677409625\n",
            "State: (2, 0), Action: (1, 0), Q-value: 0.5215438865882117\n",
            "State: (3, 1), Action: (1, 0), Q-value: -0.2847790382547765\n",
            "State: (4, 1), Action: (0, 1), Q-value: 0.8868346917251466\n",
            "State: (3, 2), Action: (1, 0), Q-value: -0.38831050015870694\n",
            "State: (4, 2), Action: (0, 1), Q-value: 0.8784233454094308\n",
            "State: (0, 0), Action: (-1, 0), Q-value: -0.742279937199843\n",
            "State: (-1, 0), Action: (0, 1), Q-value: -0.002999622846145485\n",
            "State: (-1, 0), Action: (0, -1), Q-value: -0.1\n",
            "State: (3, 0), Action: (-1, 0), Q-value: 0.4166777007877049\n",
            "State: (2, -1), Action: (-1, 0), Q-value: 0.27507447930269546\n",
            "State: (3, 2), Action: (0, -1), Q-value: 0.6468832652059731\n",
            "State: (-1, 2), Action: (0, -1), Q-value: -0.0012069379376982534\n",
            "State: (-1, 0), Action: (1, 0), Q-value: 0.2988648879909345\n",
            "State: (-1, 1), Action: (0, -1), Q-value: 0.04459230900268658\n",
            "State: (-1, 1), Action: (1, 0), Q-value: -0.2286049706410217\n",
            "State: (1, 3), Action: (1, 0), Q-value: 0.08814024000000001\n",
            "State: (2, 3), Action: (0, -1), Q-value: -0.1000118098\n",
            "State: (2, 3), Action: (1, 0), Q-value: 0.46855900000000006\n",
            "State: (1, -1), Action: (0, -1), Q-value: -0.304148046702529\n",
            "State: (3, 0), Action: (1, 0), Q-value: -0.7601528153158453\n",
            "State: (4, 0), Action: (0, 1), Q-value: -0.005781374792069336\n",
            "State: (-1, 1), Action: (-1, 0), Q-value: -0.20053900000000002\n",
            "State: (-1, 2), Action: (1, 0), Q-value: -8.622269503941601e-05\n",
            "State: (0, -1), Action: (-1, 0), Q-value: -0.2730224381582479\n",
            "State: (-1, 0), Action: (-1, 0), Q-value: -0.19002615612593537\n",
            "State: (2, 0), Action: (-1, 0), Q-value: 0.25675608389532556\n",
            "State: (3, 1), Action: (0, -1), Q-value: 0.3683058940825885\n",
            "State: (3, 0), Action: (0, -1), Q-value: -0.5981379662242201\n",
            "State: (3, -1), Action: (0, 1), Q-value: 0.6305069806706138\n",
            "State: (4, 1), Action: (1, 0), Q-value: -0.09240145650100001\n",
            "State: (3, 2), Action: (-1, 0), Q-value: -0.38255553455311386\n",
            "State: (0, -1), Action: (0, -1), Q-value: -0.30343097498931426\n",
            "State: (0, -1), Action: (1, 0), Q-value: 0.3641762669095675\n",
            "State: (3, 1), Action: (-1, 0), Q-value: -0.959061575175537\n",
            "State: (2, 1), Action: (-1, 0), Q-value: -0.27040004685817975\n",
            "State: (4, 1), Action: (-1, 0), Q-value: -0.2069294943323725\n",
            "State: (4, 0), Action: (0, -1), Q-value: -0.16904163020361251\n",
            "State: (4, 0), Action: (1, 0), Q-value: -0.2791\n",
            "State: (4, 0), Action: (-1, 0), Q-value: 0.3570034242279335\n",
            "State: (4, 2), Action: (-1, 0), Q-value: -0.07072806041596262\n",
            "State: (3, -1), Action: (1, 0), Q-value: -0.1381281550545691\n",
            "State: (1, -1), Action: (-1, 0), Q-value: 0.02056774957378428\n",
            "SARSA Agent reached the goal in 7 steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BCl1F7Q_s-L6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}