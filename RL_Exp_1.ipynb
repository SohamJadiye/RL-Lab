{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:**\n",
        "*To implement the Python program to Implementing a simple grid-world environmentand training an agent using basic Q learning*"
      ],
      "metadata": {
        "id": "ZgAL1h5b_euL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1RkXD326cha"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, rows, cols, start, goal, obstacles):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.obstacles = obstacles\n",
        "        self.state = start\n",
        "        self.is_terminal = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start\n",
        "        self.is_terminal = False\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = tuple(np.array(self.state) + np.array(action))\n",
        "\n",
        "        if next_state == self.goal:\n",
        "            reward = 1\n",
        "            self.is_terminal = True\n",
        "        elif next_state in self.obstacles or not (0 <= next_state[0] < self.rows) or not (0 <= next_state[1] < self.cols):\n",
        "            reward = -1\n",
        "            self.is_terminal = False\n",
        "        else:\n",
        "            reward = 0\n",
        "            self.state = next_state\n",
        "            self.is_terminal = False\n",
        "\n",
        "        return next_state, reward, self.is_terminal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.actions = actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_values = {}\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            action_index = np.random.choice(len(self.actions))\n",
        "            return self.actions[action_index]\n",
        "        else:\n",
        "            q_vals = [self.get_q_value((state, a)) for a in self.actions]\n",
        "            return self.actions[np.argmax(q_vals)]\n",
        "\n",
        "    def get_q_value(self, sa_pair):\n",
        "        return self.q_values.get(sa_pair, 0)\n",
        "\n",
        "    def update_q_value(self, sa_pair, new_q_value):\n",
        "        self.q_values[sa_pair] = new_q_value\n",
        "\n",
        "    def print_q_values(self):\n",
        "        print(\"Q-values Table:\")\n",
        "        for state_action, value in self.q_values.items():\n",
        "            state, action = state_action\n",
        "            print(f\"State: {state}, Action: {action}, Q-value: {value}\")"
      ],
      "metadata": {
        "id": "6NG3LjfK-5Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_q_learning(agent, environment, episodes):\n",
        "    for episode in range(episodes):\n",
        "        environment.reset()\n",
        "        state = environment.state\n",
        "\n",
        "        while not environment.is_terminal:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, is_terminal = environment.step(action)\n",
        "\n",
        "            best_next_action = max([(agent.get_q_value((next_state, a)), a) for a in agent.actions])[1]\n",
        "            new_q_value = (1 - agent.alpha) * agent.get_q_value((state, action)) + agent.alpha * (reward + agent.gamma * agent.get_q_value((next_state, best_next_action)))\n",
        "            agent.update_q_value((state, action), new_q_value)\n",
        "\n",
        "            state = next_state\n"
      ],
      "metadata": {
        "id": "JI1ZUua7-5bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    rows, cols = 4, 4\n",
        "    start = (0, 0)\n",
        "    goal = (3, 3)\n",
        "    obstacles = [(1, 1), (2, 1), (2, 2)]\n",
        "\n",
        "    environment = GridWorld(rows, cols, start, goal, obstacles)\n",
        "    actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up\n",
        "\n",
        "    agent = QLearningAgent(actions)\n",
        "\n",
        "    episodes = 1000\n",
        "    train_q_learning(agent, environment, episodes)\n",
        "\n",
        "    # Print the learned Q-values\n",
        "    agent.print_q_values()\n",
        "    # print(sa_pair)\n",
        "\n",
        "    # Test the trained agent\n",
        "    environment.reset()\n",
        "    state = environment.state\n",
        "    steps = 0\n",
        "\n",
        "    while not environment.is_terminal and steps < 20:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, _, _ = environment.step(action)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"Agent reached the goal in {steps} steps.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9mpVMnR8RiY",
        "outputId": "57a78a4b-eeb8-490c-8f0b-ae849c7c178d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values Table:\n",
            "State: (0, 0), Action: (0, 1), Q-value: 0.5904899999999987\n",
            "State: (0, 1), Action: (0, 1), Q-value: 0.6560999999999991\n",
            "State: (0, 2), Action: (0, 1), Q-value: 0.7289999999999993\n",
            "State: (0, 3), Action: (-1, 0), Q-value: -0.5239723697585628\n",
            "State: (-1, 3), Action: (0, 1), Q-value: -0.1\n",
            "State: (0, 4), Action: (0, 1), Q-value: -0.1\n",
            "State: (0, 4), Action: (0, -1), Q-value: 0.5788282442128364\n",
            "State: (0, 3), Action: (0, 1), Q-value: -0.6004052720784738\n",
            "State: (0, 3), Action: (0, -1), Q-value: 0.6002912982290319\n",
            "State: (-1, 3), Action: (0, -1), Q-value: 0.6085684006093857\n",
            "State: (0, 2), Action: (1, 0), Q-value: 0.557294410676785\n",
            "State: (1, 2), Action: (0, 1), Q-value: 0.1538999999999999\n",
            "State: (1, 3), Action: (0, 1), Q-value: -0.6712545367591679\n",
            "State: (1, 4), Action: (0, 1), Q-value: -0.18061532878833764\n",
            "State: (1, 4), Action: (0, -1), Q-value: 0.48213537995833816\n",
            "State: (1, 3), Action: (0, -1), Q-value: 0.5472295540679285\n",
            "State: (1, 2), Action: (0, -1), Q-value: -0.6843948928726448\n",
            "State: (1, 1), Action: (0, 1), Q-value: 0.6195060850917378\n",
            "State: (1, 1), Action: (0, -1), Q-value: -0.03685590000000012\n",
            "State: (1, 2), Action: (1, 0), Q-value: -0.6052895110000001\n",
            "State: (2, 2), Action: (0, 1), Q-value: -0.16561000000000003\n",
            "State: (2, 2), Action: (-1, 0), Q-value: 0.07796309023894582\n",
            "State: (1, 3), Action: (-1, 0), Q-value: 0.6766455119167599\n",
            "State: (0, 3), Action: (1, 0), Q-value: 0.8099999999999993\n",
            "State: (1, 2), Action: (-1, 0), Q-value: 0.6559208329209893\n",
            "State: (0, 2), Action: (0, -1), Q-value: 0.5267767965536758\n",
            "State: (0, 1), Action: (1, 0), Q-value: -0.4985607684916583\n",
            "State: (0, 2), Action: (-1, 0), Q-value: -0.47808330615698486\n",
            "State: (-1, 2), Action: (0, 1), Q-value: 0.6820145646993668\n",
            "State: (1, 3), Action: (1, 0), Q-value: 0.8999999999999994\n",
            "State: (2, 3), Action: (0, 1), Q-value: -0.30680188945855835\n",
            "State: (2, 4), Action: (0, 1), Q-value: -0.1\n",
            "State: (2, 4), Action: (0, -1), Q-value: -0.1729729\n",
            "State: (2, 4), Action: (1, 0), Q-value: 0.901522909781639\n",
            "State: (0, 1), Action: (-1, 0), Q-value: -0.4664476406019417\n",
            "State: (-1, 1), Action: (0, 1), Q-value: 0.635814271688038\n",
            "State: (2, 2), Action: (0, -1), Q-value: -0.1\n",
            "State: (2, 3), Action: (0, -1), Q-value: -0.44430847684412395\n",
            "State: (2, 2), Action: (1, 0), Q-value: 0.7420124921260784\n",
            "State: (0, 4), Action: (1, 0), Q-value: 0.08099950583133315\n",
            "State: (2, 3), Action: (1, 0), Q-value: 0.9999999999999994\n",
            "State: (0, 1), Action: (0, -1), Q-value: 0.4830920867201032\n",
            "State: (0, 0), Action: (0, -1), Q-value: -0.5464455435496258\n",
            "State: (0, -1), Action: (0, 1), Q-value: 0.5581761753008365\n",
            "State: (0, 0), Action: (1, 0), Q-value: 0.004244675864624962\n",
            "State: (1, 0), Action: (0, 1), Q-value: -0.1\n",
            "State: (1, 1), Action: (1, 0), Q-value: -0.2377176764882741\n",
            "State: (2, 0), Action: (0, 1), Q-value: -0.1\n",
            "State: (2, 1), Action: (0, 1), Q-value: -0.1\n",
            "State: (2, 1), Action: (0, -1), Q-value: -0.1\n",
            "State: (2, -1), Action: (0, 1), Q-value: -0.1\n",
            "State: (2, 1), Action: (1, 0), Q-value: -0.1\n",
            "State: (3, 0), Action: (0, 1), Q-value: 0.20876906952513177\n",
            "State: (3, 1), Action: (0, -1), Q-value: 0.0002697300000000001\n",
            "State: (3, 1), Action: (0, 1), Q-value: 0.4863317730407152\n",
            "State: (3, 2), Action: (0, 1), Q-value: 0.8332281830033343\n",
            "State: (0, 0), Action: (-1, 0), Q-value: -0.5297303050446301\n",
            "State: (-1, 0), Action: (0, 1), Q-value: 0.5679389925361148\n",
            "State: (2, 3), Action: (-1, 0), Q-value: 0.7100911762035488\n",
            "State: (1, 1), Action: (-1, 0), Q-value: -0.06633740590221043\n",
            "State: (1, 0), Action: (0, -1), Q-value: -0.1\n",
            "State: (1, -1), Action: (0, 1), Q-value: -0.09482977306303864\n",
            "State: (2, 0), Action: (0, -1), Q-value: -0.27036663111166326\n",
            "State: (2, -1), Action: (0, -1), Q-value: -0.1\n",
            "State: (2, -1), Action: (1, 0), Q-value: 0.025122905140629585\n",
            "State: (3, 0), Action: (0, -1), Q-value: -0.3418899949306\n",
            "State: (3, -1), Action: (0, 1), Q-value: 0.05796954316803818\n",
            "State: (-1, 2), Action: (1, 0), Q-value: 0.04537233809098318\n",
            "State: (-1, 3), Action: (1, 0), Q-value: 0.1538999999986998\n",
            "State: (1, 0), Action: (1, 0), Q-value: 0.02095359752966077\n",
            "State: (2, 0), Action: (1, 0), Q-value: 0.07007736132784116\n",
            "State: (0, -1), Action: (0, -1), Q-value: -0.07343487176714784\n",
            "State: (3, 2), Action: (-1, 0), Q-value: -0.07186160759497771\n",
            "State: (4, 2), Action: (0, 1), Q-value: 0.19\n",
            "State: (0, -1), Action: (-1, 0), Q-value: -0.05572773234164128\n",
            "State: (-1, 0), Action: (-1, 0), Q-value: -0.05404208419673029\n",
            "State: (-1, 0), Action: (0, -1), Q-value: -0.05956853980145084\n",
            "State: (3, 2), Action: (0, -1), Q-value: 0.02914593710208841\n",
            "State: (-1, 1), Action: (-1, 0), Q-value: -0.05763654574089699\n",
            "State: (3, 0), Action: (-1, 0), Q-value: 0.0036801710901562057\n",
            "State: (3, 1), Action: (-1, 0), Q-value: -0.1\n",
            "State: (4, 1), Action: (0, 1), Q-value: 0.0714697981114816\n",
            "State: (3, -1), Action: (0, -1), Q-value: -0.0983331352306\n",
            "Agent reached the goal in 8 steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mWjHvVJzk9v9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}